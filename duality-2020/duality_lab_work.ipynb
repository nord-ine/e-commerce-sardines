{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "duality_lab_work.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EegfvkoRIPo"
      },
      "source": [
        "# Duality in machine learning lab work\n",
        "\n",
        "*Notebook prepared by Mathieu Blondel, November 2020. The accompanying slides are available [here](https://www.mblondel.org/teaching/duality-2020.pdf).*\n",
        "\n",
        "In this lab work, we are going to implement block dual coordinate ascent (BDCA) when using squared L2 norm regularization and either the squared loss or the multiclass hinge loss. See the last two pages of the slides above for a recap of the necessary equations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDStDYBCSKpj"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Implement BDCA for the case of squared L2 norm regularization and squared loss. Make sure that the duality gap is decreasing. If not, there is either a bug in your gap computation or in your algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-6n8MGIRGB0"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "def primal_dual_link_squared_l2(X, Y, lam, beta):\n",
        "  \"\"\"\n",
        "  Computes primal-dual link in the squared L2 regularization case.\n",
        "\n",
        "  Args:\n",
        "    X: feature matrix (n_samples x n_features)\n",
        "    Y: label matrix (n_samples x n_classes)\n",
        "    lam: lambda value (regularization strength)\n",
        "    beta: dual variables (n_samples x n_classes)\n",
        "\n",
        "  Returns:\n",
        "    W = nabla G^*(X^T (Y - beta))\n",
        "  \"\"\"\n",
        "  \n",
        "  return  # Your code here\n",
        "\n",
        "\n",
        "def prox_squared_loss(eta, tau):\n",
        "  \"\"\"\n",
        "  Prox operator associated with the squared loss.\n",
        "\n",
        "  Args:\n",
        "    eta: input\n",
        "    tau: multiplication factor\n",
        "  \"\"\"\n",
        "  return  # Your code here\n",
        "\n",
        "\n",
        "def bdca_squared_loss(X, Y, lam, n_epochs=100):\n",
        "  \"\"\"\n",
        "  BDCA for the squared loss with squared L2 regularization (ridge regression).\n",
        "\n",
        "  Args:\n",
        "    X: feature matrix (n_samples x n_features)\n",
        "    Y: label matrix (n_samples x n_classes)\n",
        "    lam: lambda value (regularization)\n",
        "    n_epochs: number of iterations to perform\n",
        "\n",
        "  Returns:\n",
        "    W\n",
        "  \"\"\"\n",
        "  n_samples, n_classes = Y.shape\n",
        "\n",
        "  # Initialization.\n",
        "  beta = np.ones((n_samples, n_classes)) / n_classes\n",
        "  W = primal_dual_link_squared_l2(X, Y, lam, beta)  # n_features x n_classes\n",
        "\n",
        "  # Pre-compute squared norms.\n",
        "  sqnorms = np.sum(X ** 2, axis=1)\n",
        "\n",
        "  # Loop over epochs.\n",
        "  for it in range(n_epochs):\n",
        "\n",
        "    # Loop over samples / blocks.\n",
        "    for i in range(len(X)):\n",
        "      # We skip empty samples.\n",
        "      if sqnorms[i] == 0:\n",
        "        continue\n",
        "\n",
        "      # Update beta[i] here.\n",
        "      # Your code here.\n",
        "\n",
        "      # Recompute W.\n",
        "      W = primal_dual_link_squared_l2(X, Y, lam, beta)\n",
        "\n",
        "    # Compute the duality gap once per epoch.\n",
        "    # Your code here.\n",
        "    #print(gap)\n",
        "\n",
        "  return W\n",
        "\n",
        "# Load toy data\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "# Transform labels to a one-hot representation.\n",
        "# Y has shape (n_samples, n_classes).\n",
        "Y = LabelBinarizer().fit_transform(y)\n",
        "\n",
        "# Shuffle the samples.\n",
        "rng = np.random.RandomState(0)\n",
        "perm = rng.permutation(len(X))\n",
        "X = X[perm]\n",
        "Y = Y[perm]\n",
        "\n",
        "bdca_squared_loss(X, Y, lam=1000, n_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yeRCkUyUsEy"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Implement the same algorithm for the multiclass hinge loss. Basically the structure remains the same. Only the proximity operator and the duality gap computation change. Since it is needed for the proximity operator, I provide a routine for computing the projection onto the simplex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuXGhsMrVUWB"
      },
      "source": [
        "def projection_simplex(v, z=1):\n",
        "  \"\"\"\n",
        "  Project v onto the probability simplex.\n",
        "  \"\"\"\n",
        "  n_features = v.shape[0]\n",
        "  u = np.sort(v)[::-1]\n",
        "  cssv = np.cumsum(u) - z\n",
        "  ind = np.arange(n_features) + 1\n",
        "  cond = u - cssv / ind > 0\n",
        "  rho = ind[cond][-1]\n",
        "  theta = cssv[cond][-1] / float(rho)\n",
        "  w = np.maximum(v - theta, 0)\n",
        "  return w\n",
        "\n",
        "def prox_multiclass_hinge_loss(eta, tau, v):\n",
        "  \"\"\"\n",
        "  Prox operator associated with the multiclass hinge loss.\n",
        "\n",
        "  Args:\n",
        "    eta: input\n",
        "    tau: multiplication factor\n",
        "    v: input (see slides for definition)\n",
        "  \"\"\"\n",
        "  return  # Your code here\n",
        "\n",
        "def bdca_multiclass_hinge_loss(X, Y, lam, n_epochs=100):\n",
        "  \"\"\"\n",
        "  BDCA for the multiclass hinge loss with squared L2 regularization.\n",
        "\n",
        "  Args:\n",
        "    X: feature matrix (n_samples x n_features)\n",
        "    Y: label matrix (n_samples x n_classes)\n",
        "    lam: lambda value (regularization)\n",
        "    n_epochs: number of iterations to perform\n",
        "\n",
        "  Returns:\n",
        "    W\n",
        "  \"\"\"\n",
        "  n_samples, n_classes = Y.shape\n",
        "\n",
        "  # Initialization.\n",
        "  beta = np.ones((n_samples, n_classes)) / n_classes\n",
        "  W = primal_dual_link_squared_l2(X, Y, lam, beta)  # n_features x n_classes\n",
        "\n",
        "  # Pre-compute squared norms.\n",
        "  sqnorms = np.sum(X ** 2, axis=1)\n",
        "\n",
        "  # Loop over epochs.\n",
        "  for it in range(n_epochs):\n",
        "\n",
        "    # Loop over samples / blocks.\n",
        "    for i in range(len(X)):\n",
        "      # We skip empty samples.\n",
        "      if sqnorms[i] == 0:\n",
        "        continue\n",
        "\n",
        "      # Update beta[i] here\n",
        "      # Your code here.\n",
        "\n",
        "      # Recompute W.\n",
        "      W = primal_dual_link_squared_l2(X, Y, lam, beta)\n",
        "\n",
        "    # Compute the duality gap once per epoch.\n",
        "    # Your code here.\n",
        "    #print(gap)\n",
        "\n",
        "  return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H3oBUH9WO2n"
      },
      "source": [
        "## Bonus exercise\n",
        "\n",
        "Add support for elastic-net regularization."
      ]
    }
  ]
}